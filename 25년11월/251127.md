# 251127

### HTTP persistent Connnection

- HTTP 1.0 
  - TCP 3 way HandShake 연결 
  - 기본적으로는 서버에서 HTTP Response 이후에 TCP연결도 종료
  - 그러나, Header에 'Connection: keep-alive'가 있다면?
    - HTTP Response가 내려와도 TCP 연결자체는 끊기지 않음.
      - 선택적으로 'Keep-Alive: timeout5;, max=100'와 같이 설정 가능
        - 5초 동안 최대 100개의 HTTP요청을 맺겠다는 의미
    - TCP는 그대로 유지되고 HTTP 메시지를 여러번 흘려보낸다.
      - 클라는 Content-Length, Connection: Close(서버에서 Connection을 종료하겠다는 의미)를 보고 응답의 범위를 추정
      - TCP 레벨에서는 모든게 바이트 스트림 하나로 처 됨.
- HTTP 1.1
  - keep-alive가 기본값
  - 즉 persistent connection
    - Header에 'Connection: keep-alive'를 명시할 필요 X



다음은 HTTP 1.0 / HTTP 1.1 스펙에서 클라 - 서버간 요청과 응답이 맺어지는 간단한 예시

```
1. TCP 연결 수립
	커널이 listen 중인 소켓에 대해서 accept()호출
	새 소켓(파일 디스크립터 fd)이 생기고 ESTABLISHED
2. HTTP Request 요청 수신
	워크 스레드 1개가 해당 소켓을 맡아서 read()를 블로킹
	클라이언트가 HTTP Request 바이트를 보내면, 
	커널이 TCP 패킷을 조립해서 커널 버퍼에 쌓는다.
	read()가 꺠어나서 바이트를 읽는다.
	WAS는 해당 바이트 스트림을 파싱하고 
	Request Line(), 
	headers,
	body를 읽는다.
3. 애플리케이션 로직 실행
4. HTTP REsponse 전송
	컨트롤러가 응답을 생성하고, WAS가 HTTP Header+Body를 다시 바이트로 만들어서 
	write()호출
	커널이 해당 바이트를 TCP 패킷으로 쪼개서 네트워크로 보낸다.
5. Connection :Close라면,
	응답을 다 보내고 나서 서버가 close(fd)를 호출
	커널은 TCP FIN을 보내고 연결이 종료
	워커 스레드는 다시 스레드풀로 복귀하거나 다른 소켓을 처리하러 감
	keep-alive / persistent의 경우
	서버가 close()를 안한다.
	커널은 TCP 소켓을 ESTABLISHED상태로 유지
	워커 스레드는 다시 read()상태로 복귀
		-> 애플리케이션의 thread는 idle상태
```



- 중요한 점은 HTTP가 맺어지기 전에 TCP가 맺어지고 1.1 스펙부터는 TCP가 맺어진 이상 TCP 연결이 끊기기 전까지 '소켓'을 점유하고 있다.
- 즉, Websocket만이 소켓(파일 디스크립터 fd)만을 계속해서 점유하고 있는 것이 아니라, HTTP 1.1스펙부터는 모든 통신 자체가 항상 1회적으로 연결/종료가 아니기 때문에 서버는 소켓을 close()하지 않고 TCP 소켓을 계속 물고있음
  - 따라서 서비스 특성마다, 상이(아래는 Keep-Alive를 나누는 예시)
    - 일반 웹 / RESTAPI서버
      - Short Keep-Alive
        - 동시접속자 수를 늘리고 스레드 고갈 방지를 위해 설정한다고함
    - 처리량 을 높이고 지연시간 낮추는게 중요한 경우
      - ex) 결제와 같이 내부적으로 많은 서비스의 API를 호출해야하는 경우
        - 각 호출마다 TCP HandShake가 발생
        - Keep-Alive를 길게하면 서비스 간 영구적 연결 채널 유지가능
        - TCP 연결 및 해제에 리소스 소모 X
        - 통상 MicroService와 같이 나눌경우 VPC와 같이 하나의 내부 네트워크에 대역을 유지하므로 효율성 극대화 가능.
  - 그러나, Websocket과 달리 HTTP 'keep-alive'는 timeout등으로 연결 종료의 상한이 존재
  - 서비스(어플리케이션)마다 운영시에 해당 시간을 다르게 설정할 수 있음.
    - Nginx
      - keepalive_timeout 설정 가능
    - Tomcat
      - connectionTimeout, keepAliveTimeout 설정가능

### 그렇다면 PersistentConnection이 계속 맺어지면 Thread는 서버에서 어떻게 될까?

즉 Persistent Connection에서 별도로 KeepAlive 시간을  적절하게 설정하지 않는다면, Application에서 Idle Thread가 늘어나는데, ThreadPool 내부의 쓰레드가 모두 점유되어버리면 금새 고갈이 되버리지 않을까?



만약 그렇다면 Keep-Alive자체는 항상 짧게 유지해야하는거 아닐까?

그런데 전술했듯이 서비스 특성마다 HTTP 사용시에 keep-alive사용전략이 서버에서 다를텐데 이걸 어떻게 구성하는걸까? 



-> 여기서부턴 BIO Connector, NIO Connector에 대한 개념이 필요



### BIO Connector와 NIO Connector



> “persistent connection이면 WAS 입장에서 워커 스레드가 idle로 많아지는 거 아냐?”

이건 이렇게 정리할 수 있어:

- **BIO Connector 기준**
  - ✅ “그렇다”
  - 소켓에 `read()`로 묶여 있는 worker가 idle 상태로 스레드를 점유함
  - keep-alive를 길게 가져가면 스레드가 다 물려서 새 요청을 못 받는 상황이 실제로 발생
- **NIO Connector 기준 (요즘 기본)**
  - ❌ “idle connection이 worker thread를 점유하지 않는다”
  - idle인 건 소켓 + Poller 관리 엔트리 정도이고,
  - worker는 요청 처리 동안에만 빌렸다가 다시 반납됨







https://jh-labs.tistory.com/329

https://github.com/spring-projects/spring-boot/issues/36087

https://velog.io/@sihyung92/how-does-springboot-handle-multiple-requests

https://jh-labs.tistory.com/353

https://www.youtube.com/watch?v=VhSu1pRIEqQ







CF) 하단의 HTTP/2와 3는 참고

- HTTP/2
  - TCP 위에서 동작
  - 스트림 개념을 이용해서 하나의 TCP위에서 여러 요청과 응답을 멀티플레싱
    - Q) 위 개념은 아직 명확히 이해가 안감.
  - 헤더 압축(Hpack), 바이너리 프레이밍
- HTTP/3
  - TCP 대신 QUIC(UDP 기반)
  - Q)이것도 아직 이해가 명확히 가진 않음
- WebSocket
  - 



https://velog.io/@cjh8746/%EC%95%84%ED%8C%8C%EC%B9%98-%ED%86%B0%EC%BA%A3%EC%9D%98-NIO-Connector-%EC%99%80-BIO-Connector%EC%97%90-%EB%8C%80%ED%95%B4-%EC%95%8C%EC%95%84%EB%B3%B4%EC%9E%90

---



```

오답이 되는 반례 (이것도 꼭 알아두세요!)

“HTTP/2나 HTTP/3 쓰면 WebSocket만큼 빠르지 않나요?”
→ HTTP/2 멀티플렉싱 + 헤더 압축(HPACK)으로 훨씬 좋아지지만, 여전히 요청-응답 패턴이라 실시간 양방향은 불가능하고, 서버에서 먼저 push하려면 Server Push를 써야 하는데 제한 많음.
→ HTTP/3 (QUIC)도 동일한 논리적 한계 있음.

“그럼 WebSocket이 항상 더 좋나요?”
→ 아니요. 연결 유지 비용(메모리, 파일 디스크립터), NAT 타임아웃, 모바일 백그라운드 종료 등의 문제 때문에 장시간 idle한 수백만 연결은 WebSocket이 오히려 독이 됨 → 그럴 땐 SSE + Polling fallback 조합이 더 나을 수 있음.
```

