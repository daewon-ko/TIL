#  251111



### Multi Agent LLM

- 1대의 LLM만을 사용하는 것이 아니라, 각자의 역할에 따라 LLM을 분업화시켜서 팀처럼 사용하는 것

![multi-agent llm components](https://cdn.prod.website-files.com/614c82ed388d53640613982e/66bc561d889375519710d3d7_668c047a69dcf02f9faec05c_multi-agent-llm-components.webp)

- Agent Orchestration
  - 사용자 질의(Query)를 여러 에이전트로 분해 및 연결 수행
  
- Agents
  - prompt, LLM, Tools라는 각 구성요소를 통해서 행동
  
- Human in the loop
  - 완전 자동화된 에이전트 시스템 X
  - ex) 어떤 Agent를 호출할지 선택 / 다음 단계의 작업을 지시(피드백 루프) 등
    - <- Cursor / Copilot등의 예시를 떠오르면 이해가 쉬울듯

- 사용자의 복잡한 질의를 작은 subTask로 나누고 적절한 agents에게 역할등을 할당한다. 
  - agents들은 각각의 LLM을 통해 reasoning을 수행하고 tools를 통해 추론 이후의 작업을 수행한다.
    - ex) 여행 Multi Agent Team을 상정하면, 교통 agent는 차, 셔털브서, 기차 등과 같이 교통과 연관된 모든 것을 총괄할 것이다.
    - 그 과정에서 MCP_tools를 지속해서 호출한다.
  
- Singe Agent VS Multi-Agent LLMS
  1. 정확도와 할루시네이션 측면에서 Multi Agent가 낫다.
  
     - Agents끼리 각자의 업무를 체크함으로써 할루시네이션 확률을 낮추고 시스템 신뢰성을 향상시킴
  
     - LLM-Fine-Tuning 역시 이것에 크게 일조할 수 있음
  
  2. 확장된 컨텍스트 다루기 가능
  
     - 단일 Agent가 많은 양의 정보와 대화를 다루는 것은 불가능
     - Multi Agent는 Context를 쪼개서 각 Agent가 각자의 Segment만 다룸으로써 더 명료하게 분석 가능
  
  3. 효율성과 멀티태스킹
  
  4. 협력성
  
  
  
  다음은 LLM Framework중 Multi-Agent를 사용하는 예시()
  
  1. AutoGen
  2. LangChain
  3. LangGraph
  4. CrewAi
  5. AutoGPT
  
  등.. 굉장히 많은 예시가 존재한다.
  
  (참고: Multi-Agent를 사용할 수 있다는거지 위의 프레임워크들이 Mutl-Agent를 반드시 사용한다는 개념은 아님)
  
  

- 한계도 존재
  - 태스크 할당
    - 어떤 Agent에게 어떤 Task를 내려주는 것은 쉬운 일은 X
  - 협력 추론
    - 함께 debate, reasoning하는 것은 쉬운일 X
  - 컨텍스트 관리
    - 에이전트들끼리의 모든 정보와 대화를 모두 트래킹 하는 것은 쉬운 일 X
  - 시간과 비용
    - 에이전트들끼리의 상호작용은 시간과 상당한 리소스를 소모한다.



참고 

https://www.superannotate.com/blog/multi-agent-llms

https://medium.com/corca/llm-multi-agent-customer-service%EB%A5%BC-%EA%B8%B0%EA%B9%94%EB%82%98%EA%B2%8C-%EC%9E%90%EB%8F%99%ED%99%94%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95-2eaec7654385





---



### RAG와 AI

- LLM의 근본적한계
  - LLM의 지식은 학습 종료 시점에 고정
    - 모델에 따라 다르지만, 인터넷에 연결되지 않으면 지식은 멈춰있음
  - private Company Data에 접근 불가
    - 조직 내부정보, 상품 상세 등 공적 모델의 훈련 데이터가 아니면 LLM은 알 수 없음
  - 할루시네이션
  - 컨텍스트가 있더라도, Generic한 답변을 내뱉는 경우가 있음

-> 위 한계를 RAG로 해결

- RAG는 
  - Specific한 Data에 대한 접근을 LLM에게 열어줌으로써 이를 가능하게 함
  - LLM은 응답 전에 대해서 해당  DataSet에 대한 접근 후 응답을 클라이언트에게 내려줌

- Rag는 크게 2가지 층위가 엮여있음
  - 연관된 정보를 documents of Collection에서 검색
  - 해당 정보를 이용해서 정확한 응답을 생성
  - RAG 시스템의 동작방식을 모사한다.
    - 1. 사용자가 프롬프트+ 질의를 Chat UI형태로 입력한다.
      2. 질의에 대해서 Knowledge Source(PDF, DB, Documents 등..)을 조회한다. 
      3. 연관정보를 서버에 Return한다.
      4. 서버에서 프롬프트 + 질의 + Knowled Source가 리턴한 연관정보를 함께 LLM에게 전달한다.
      5. LLM은 Text를 서버에 Return한다.
      6. 서버에서 사용자에게 응답을 Return한다.

![img](https://substackcdn.com/image/fetch/$s_!L2aN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb51ae79-2f3a-4707-bd45-7d6592984da0_1600x963.png)

- 단일 LLM보다 RAG를 도입하면 좋은 경우는 다음과 같다.
  - 사용사례가 제품 재고, 가격 , 뉴스와 같이 자주 변경되는 정보를 포괄할 때
  - 모델의 훈련 데이터에 포함되지 않은 내부문서 등과 같은 사적 정보를 다룰때
  - 법률, 의료, 금융과 같이 정확성이 아주 중요해서 할루시네이션을 반드시 포함하면 안될 때
  - 정보의 출처를 증명하거나 인용을 제공하는 것이 중요할때, 표준 LLM 응답으로는 불가능한 투명성과 감사 가능성을 제공
  - 응용프로그램이 모든 프롬프트에 포함시키기에는 비실용적인 문서 컬렉션을 처리할때
- 반대로 이야기 하면, 일반적인 지식에 대한 질의의 경우 LLM이 꽤나 정상적으로 답변이 가능하다.



- RAG의 동작 방식
  - 분명히 구별되는 2가지 층위에서 동작
  - Document Preparation
    - 시스템을 준비할때 발생
    - 새로운 document Or Knowledge Source가 추가될때마다 이어서 발생가능
  - Query Processing
    - 사용자가 '리얼타임'으로 질의시마다 발생

###  Document Preparation

- 사용자 질의가 도착하기 전에 발생하며 여러 절차를 포괄함.
  - 1. 문서가 모여야하고, 처리되어야함.
       - 각 문서(PDF, 웹페이지, 단어나열이든.. DB Record이든)들은 Plain Text로 변환되어야함
       - 해당 과정에서 다양한 포맷이 일원화되고 실제 Data가 정화됨.
    2. Text를 작은 청크 단위로 쪼갬
       - Chunk단위로 쪼개지 않으면 Plain Text는 대부분 처리하기 어렵기 때문에 청킹 작업을 수행
         - 청크가 너무 작으면 Context가 부족하고 반대로 청크가 너무크면 부정확해짐
         - 대부분의 시스템은 500 -1000단어 수준으로 청크를 쪼개며 경계만 문맥 유지를 위해 연속된 청크간 일부 중복이 발생한다.
    3. 텍스트 조각들을 벡터표현(임베딩)으로 변환
       - 각 청크들은 연속된 숫자로 문장의 의미가 변환됨
         - ex) '분기별 재무 보고서'라는 문장에 대한 청크는 [0.23, -0.45...]와 같이 수 백, 수 천 차원의 벡터로 변환 가능
       - 해당 숫자들을 수학적으로 비교가능하도록 테스트의 의미를 인코딩한다.
       - 유사한 개념은 유사한 숫자 패턴을 생성하므로, 서로 다른 단어가 사용되더라도 시스템이 관련 컨텐츠를 찾을 수 있음
    4. 임베딩된 청크들은 Vector DB에 저장됨
       - Vector DB는 유사한 벡터를 찾는데 최적화되어있음
       - 신속한 유사도 검색이 가능하도록 임베딩 자체를 Indexing(색인화)함
       - 일반적으로 출처문서, 페이지 번호, 타임스탬프 등과 같은 '메타데이터'가 청크와 함께 저장이 된다.

### User Query Processing

![img](https://substackcdn.com/image/fetch/$s_!AOWs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2d6bc63-7e1c-4958-85f9-3c3ae2018ae1_1600x1016.png)

- 1. 사용자 질의가 들어온다.
  2. 해당 질의가 벡터로 변환된다.
     - 문서의 임베딩 과정과 동일한 과정이 수행된다.
       - ex) "What is Our fund Policy For Electronics"라는 사용자 질의에 대해서 벡터표현으로 변환된다.
  3. Vector DB 내에서 사용자 질의에 벡터변환된 값가 가장 유사도 높은 문서 청크를 조회한다.
     - Text 비교보다는 수학적 연산을 사용하기 때문에 유사도 검색의 속도가 굉장히 빠르다.
       - 여기서 사용되는 알고리즘은 수백만 청크 속에서 유사도 높은 청크를 수 밀리세컨드 내에 조회가능하게끔 한다. 
       - 시스템은 3개에서 ~10개정도의 가장 유사도 높은 청크를 조회한다.
  4. 연관성과 metaData및 비즈니스 룰을 기반으로 우선순위를 매겨서 LLM에 전달하기 위한 Context가 생성된다.
     - 예를들면 최근의 문서들이 오래된 문서들보다 우선순위가 더 높을 수 있고, 특정 소스가 다른 소스에 비해서 더 우선순위가 높을 수 있다.(설정하기 나름인 듯)
  5.  LLM은 유저의 본래 질의와 검색된 Context를 함께 전달받는다.
     - 프롬프트는 다음을 포괄한다.
       - 제공된 Context 문서
       - User의 Query
       - 제공된 Context에 근거한 답변하는 방법
       - 문맥에 없는 정보를 처리하는 지침
- 모델은 특정 정보를 Context에 포괄하고 있기때문에 응답을 일반적이기 보다는 정확하고 상세할 수 있음
- 사용자에게 응답이 전달되기 전에 후처리 과정을 거치는 경우가 많음
  - 인용추가, 가독성 향상 목적 응답 서식 조정 등이 포함 가능



### Embeddings

- 정보 검색에서 가장 중요한 문제는 사람들이 동일한 아이디어라고 해도 제 각기 표현한다는 것
  - 전통적 방식에서는, 키워드 조회를 할때 정확한 단어가 매칭되는지를 확인하였고 이러한 변화를 정확히 포착하지 못했음.
  - ex) 문서에 "The company permits product returns within 30 days"라고 적혀있었다면, 
  - 사용자가 "How long can I send items back?"라고 질의할 경우에 전통적 DB에서는 두 텍스트 간의 명확한 관계가 있음에도 키워드 서칭이 불가능했음

- 임베딩은 단순히 단어의 표면의 매칭이 아니라 문장의 의미를 포착함
  - 단순히 구체적 단어가 어떻게 쓰였냐가 아니라, 텍스트가 실제로 어떤 의미인지를 참조한다.
  - 텍스트가 임베딩으로 변환되면, 벡터값(임베딩된 결과값)은 텍스트의 의미를 나타낸다.
- 단어를 숫자로 변환하는 것은 미스테리해보이지만 원칙은 간단하다.
  - 임베딩모델이 텍스트를 읽고 수천, 수백개의 숫자 목록을 출력한다.
  - 이 숫자들은 텍스트를 다차원 공간 내의 한 점으로 위치시킨다.
  - 이는 지도 상의 좌표와 유사하다고 할 수 있고, 다만, 위경도처럼 두 차원 측이 아닌 



참고:

https://blog.bytebytego.com/p/how-rag-enables-ai-for-your-data?utm_source=publication-search